<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary"><title>Hexo</title><link ref="canonical" href="http://example.com/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Hello Stun</div><div class="header-banner-info__subtitle">An elegant theme for Hexo</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/21/Sqoop%E9%85%8D%E7%BD%AE/">Sqoop配置</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-21</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-21</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="sqoop安装"   >
          <a href="#sqoop安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop安装" class="headerlink" title="sqoop安装"></a>sqoop安装</h1>
      <p>安装sqoop的前提是已经具备java、mysql、hadoop和hive环境。</p>
<p>1、将sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz拖到node2中&#x2F;export&#x2F;software路径下</p>
<p>2、解压到&#x2F;export&#x2F;server路径下</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/software/</span><br><span class="line">tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /expoort/server</span><br></pre></td></tr></table></div></figure>

<p>3、设置软链接</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ /export/server/sqoop</span><br></pre></td></tr></table></div></figure>

<p>4、修改环境变量</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">#SQOOP_HOME</span><br><span class="line">export SQOOP_HOME=/export/server/sqoop</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br></pre></td></tr></table></div></figure>

<p>5、修改配置文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd $SQOOP_HOME/conf</span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh（sqoop-env-template.sh改名为sqoop-env.sh）</span><br><span class="line">vi sqoop-env.sh</span><br><span class="line">export HADOOP_COMMON_HOME= /export/server/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME= /export/server/hadoop</span><br><span class="line">export HIVE_HOME= /export/server/hive</span><br></pre></td></tr></table></div></figure>

<p>6、加入mysql的jdbc驱动包</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /export/server/hive/lib/mysql-connector-java-5.1.32.jar $SQOOP_HOME/lib/</span><br></pre></td></tr></table></div></figure>

<p>7、验证启动sqoop</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 本命令会列出所有mysql的数据库。</span><br><span class="line">cd /export/server/sqoop</span><br><span class="line">bin/sqoop list-databases \</span><br><span class="line"> --connect jdbc:mysql://node1:3306/ \</span><br><span class="line"> --username root --password Hadoop</span><br></pre></td></tr></table></div></figure>


        <h1 id="sqoop导入"   >
          <a href="#sqoop导入" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop导入" class="headerlink" title="sqoop导入"></a>sqoop导入</h1>
      <p>“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据。</p>

        <h2 id="sqoop测试表数据"   >
          <a href="#sqoop测试表数据" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop测试表数据" class="headerlink" title="sqoop测试表数据"></a>sqoop测试表数据</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在mysql中创建数据库userdb</span><br><span class="line">创建三张表: emp雇员表、emp_add雇员地址表、emp_conn雇员联系表</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/1.png"></p>

        <h2 id="全量导入MySQL表数据到HDFS"   >
          <a href="#全量导入MySQL表数据到HDFS" class="heading-link"><i class="fas fa-link"></i></a><a href="#全量导入MySQL表数据到HDFS" class="headerlink" title="全量导入MySQL表数据到HDFS"></a>全量导入MySQL表数据到HDFS</h2>
      <p>该命令用于从MySQL数据库服务器中的emp表导入HDFS</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#example1-mysql-hdfs-start</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--target-dir /sqoop/sqoopresult_test \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></div></figure>

<p>为了验证在HDFS导入的数据，使用以下命令查看导入的数据</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cat /sqoop/sqoopresult_test/part-m-00000</span><br></pre></td></tr></table></div></figure>

<p>在HDFS上默认用逗号,分隔emp表的数据和字段</p>
<p><img src="/../image/sqoop/2.png"></p>
<p>在web中查看</p>
<p><img src="/../image/sqoop/3.png"></p>

        <h2 id="在HDFS上用’-t’-分隔emp表的数据和字段"   >
          <a href="#在HDFS上用’-t’-分隔emp表的数据和字段" class="heading-link"><i class="fas fa-link"></i></a><a href="#在HDFS上用’-t’-分隔emp表的数据和字段" class="headerlink" title="在HDFS上用’\t’,分隔emp表的数据和字段"></a>在HDFS上用’\t’,分隔emp表的数据和字段</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#example2-mysql-hdfs-terminated</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/sqoopresult_test2 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/4.png"></p>

        <h1 id="sqoop导出"   >
          <a href="#sqoop导出" class="heading-link"><i class="fas fa-link"></i></a><a href="#sqoop导出" class="headerlink" title="sqoop导出"></a>sqoop导出</h1>
      <p>将数据从Hadoop生态体系导出到RDBMS数据库导出前，目标表必须存在于目标数据库中。</p>

        <h2 id="默认模式导出HDFS数据到mysql"   >
          <a href="#默认模式导出HDFS数据到mysql" class="heading-link"><i class="fas fa-link"></i></a><a href="#默认模式导出HDFS数据到mysql" class="headerlink" title="默认模式导出HDFS数据到mysql"></a>默认模式导出HDFS数据到mysql</h2>
      <p>1、准备HDFS数据</p>
<p>在HDFS文件系统中“&#x2F;emp&#x2F;”目录的下创建一个文件emp_data.txt：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/data/sqoop-data/emp/</span><br><span class="line">vim emp_data.txt</span><br><span class="line">1201,gopal,manager,50000,TP</span><br><span class="line">1202,manisha,preader,50000,TP</span><br><span class="line">1203,kalil,php dev,30000,AC</span><br><span class="line">1204,prasanth,php dev,30000,AC</span><br><span class="line">1205,kranthi,admin,20000,TP</span><br><span class="line">1206,satishp,grpdes,20000,GR</span><br><span class="line">#上传至hdfs</span><br><span class="line">hadoop fs -mkdir /sqoop/emp_data</span><br><span class="line">hadoop fs -put emp_data.txt /sqoop/emp_data</span><br></pre></td></tr></table></div></figure>

<p>2、手动创建mysql中的目标表</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use userdb;</span><br><span class="line"></span><br><span class="line">mysql&gt; create table employee ( </span><br><span class="line"></span><br><span class="line">  id int not null primary key, </span><br><span class="line"></span><br><span class="line">  name varchar(20), </span><br><span class="line"></span><br><span class="line">  deg varchar(20),</span><br><span class="line"></span><br><span class="line">  salary int,</span><br><span class="line"></span><br><span class="line">  dept varchar(10));</span><br></pre></td></tr></table></div></figure>

<p>3、执行导出命令</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#example10-hdfs-mysql-export</span><br><span class="line"></span><br><span class="line">bin/sqoop export \</span><br><span class="line"></span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line"></span><br><span class="line">--username root \</span><br><span class="line"></span><br><span class="line">--password hadoop \</span><br><span class="line"></span><br><span class="line">--table employee \</span><br><span class="line"></span><br><span class="line">--columns id,name,deg,salary,dept \</span><br><span class="line"></span><br><span class="line">--export-dir /sqoop/emp_data/</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/sqoop/5.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/21/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/">Kafka命令行操作</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-21</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-21</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>Kafka 中提供了许多命令行工具（位于$KAFKA HOME&#x2F;bin 目录下）用于管理集群的变更。</p>

        <h1 id="创建topic"   >
          <a href="#创建topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h1>
      <p>创建一个topic（主题）。Kafka中所有的消息都是保存在主题中，要生产消息到Kafka，首先必须要有一个确定的主题。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#基本方式</span><br><span class="line">./kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181</span><br><span class="line"></span><br><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic 名称</span><br><span class="line"></span><br><span class="line">#手动指定副本的存储位置</span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">该方式下,命令会自动判断所要创建的 topic 的分区数及副本数</span><br><span class="line"></span><br><span class="line">#bootstrap方式</span><br><span class="line"># 创建名为test的主题</span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server node1:9092 --topic test</span><br><span class="line"># 查看目前Kafka中的主题</span><br><span class="line">bin/kafka-topics.sh --list --bootstrap-server node1:9092</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 不能同时使用--partitions --replication-factor参数指定partition的AR列表，未指定AR列表则会根据负载均衡算法将partition的replica均衡的分布在Kafka集群中。</span><br><span class="line"></span><br><span class="line">--replica-assignment 1:3,2:1,3:2，</span><br><span class="line">#逗号区分不同的partition，</span><br><span class="line">#冒号区别相同partition中的replica，</span><br><span class="line">#partition-0的AR=[1,3]，partition-1的AR=[2,1]，partition-2的AR=[3,2]。</span><br><span class="line"></span><br><span class="line">Eg：testMcdull222AR列表计算出来时--replica-assignment 2:3,1:3,1:2 。数字指的是broker的ID号</span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 参数一般不由用户指定，由Kafka默认分配算法保证，有两个原则：</span><br><span class="line"></span><br><span class="line">（1）使Topic的所有Partition Replica能够均匀地分配至各个Kafka Broker（负载均衡）；</span><br><span class="line">（2）Partition 内的replica能够均匀地分配在不同Kafka Broker。</span><br><span class="line"></span><br><span class="line">#如果Partition的第一个Replica分配至某一个Kafka Broker，那么这个Partition的其它Replica则需要分配至其它的Kafka Brokers，即Partition Replica分配至不同的Broker；</span><br><span class="line"></span><br><span class="line">#分配原则</span><br><span class="line">1、从Broker随机位置开始按照轮询方式选择每个Partition的第一个replica</span><br><span class="line">2、不同Partition剩余replica按照一定的偏移量紧跟着各自的第一个replica</span><br></pre></td></tr></table></div></figure>


        <h1 id="删除topic"   >
          <a href="#删除topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#删除topic" class="headerlink" title="删除topic"></a>删除topic</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1：2181</span><br><span class="line"></span><br><span class="line">#（异步线程去删除）删除 topic,需要一个参数处于启用状态: delete.topic.enable = true,否则删不掉</span><br><span class="line"></span><br><span class="line">#使用 kafka-topics.sh 脚本删除主题的行为本质上只是在 ZooKeeper 中的 /admin/delete_topics 路径下 建一个与待删除主题同名的节点,以标记该主题为待删除的状态。与创建主题相同的是,真正删除主题的动作也是由 Kafka 的控制器负责完成的。</span><br></pre></td></tr></table></div></figure>


        <h1 id="查看topic"   >
          <a href="#查看topic" class="heading-link"><i class="fas fa-link"></i></a><a href="#查看topic" class="headerlink" title="查看topic"></a>查看topic</h1>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#(1)列出当前系统中的所有 topic </span><br><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --list</span><br><span class="line"></span><br><span class="line">#(2)查看 topic 详细信息</span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_5-6   --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">bin/kafka-topics.sh --describe --topic tpc_5-6 --zookeeper node1:2181 </span><br></pre></td></tr></table></div></figure>

<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">	 Topic: tpc_1 PartitionCount:2 ReplicationFactor:2 Configs: </span><br><span class="line">	 Topic: tpc_1 Partition: 0 Leader: 0 Replicas: 0,1 Isr: 0,1</span><br><span class="line">	 Topic: tpc_1 Partition: 1 Leader: 1 Replicas: 1,2 Isr: 1,2</span><br><span class="line">#从上面的结果中, 可以看出, topic 的分区数量, 以及每个分区的副本数量, 以及每个副本所在的 broker 节点,以及每个分区的 leader 副本所在 broker 节点,以及每个分区的 ISR 副本列表; </span><br><span class="line"></span><br><span class="line">AR=ISR+OSR</span><br><span class="line"></span><br><span class="line">ISR: in sync replicas 同步副本(当然也包含 leader 自身) -&gt;follower去找leader同步数据</span><br><span class="line"></span><br><span class="line">OSR:out of sync replicas 失去同步的副本(数据与 leader 之间的差距超过配置的阈值)</span><br><span class="line"></span><br><span class="line">#kafka不是完全同步，也不是完全异步</span><br><span class="line">1.leader会维持一个与其保持同步的replica集合，该集合就是ISR，每一个partition都有一个ISR，它是有leader动态维护。</span><br><span class="line"></span><br><span class="line">2.我们要保证kafka不丢失message，就要保证ISR这组集合存活（至少有一个存活），并且消息commit成功。</span><br></pre></td></tr></table></div></figure>

</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/20/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">Kafka环境配置</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-20</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-21</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="一、安装zookeeper"   >
          <a href="#一、安装zookeeper" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、安装zookeeper" class="headerlink" title="一、安装zookeeper"></a>一、安装zookeeper</h1>
      <p>Kafka集群是必须要有ZooKeeper</p>

        <h1 id="二、安装kadka集群"   >
          <a href="#二、安装kadka集群" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、安装kadka集群" class="headerlink" title="二、安装kadka集群"></a>二、安装kadka集群</h1>
      
        <h2 id="1、上传安装title-Kafka环境配置包"   >
          <a href="#1、上传安装title-Kafka环境配置包" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、上传安装title-Kafka环境配置包" class="headerlink" title="1、上传安装title: Kafka环境配置包"></a>1、上传安装title: Kafka环境配置包</h2>
      <p>上传kafka_2.12-2.4.1.tgz到&#x2F;export&#x2F;server目录下，解压</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">tar -zxvf kafka_2.12-2.4.1.tgz</span><br></pre></td></tr></table></div></figure>


        <h2 id="2、修改配置文件"   >
          <a href="#2、修改配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、修改配置文件" class="headerlink" title="2、修改配置文件"></a>2、修改配置文件</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 进入配置文件目录</span><br><span class="line">cd /export/server/kafka_2.11-2.0.0/config</span><br><span class="line"># 编辑配置文件</span><br><span class="line">vi server.properties</span><br><span class="line"></span><br><span class="line"># 为依次增长的:0、1、2、3、4,集群中唯一 id从0开始，每台不能重复，第一块要改的</span><br><span class="line">broker.id=0 </span><br><span class="line"></span><br><span class="line">----Logbasic------</span><br><span class="line">#数据存储的目录，第二块要改的</span><br><span class="line">log.dirs=/export/data/kafka-logs  </span><br><span class="line"></span><br><span class="line">---zookeeper----</span><br><span class="line">#指定 zk 集群地址，第四块要改的</span><br><span class="line">zookeeper.connect=node1:2181,node2:2181,node3:2181</span><br></pre></td></tr></table></div></figure>


        <h2 id="3、分发kafka"   >
          <a href="#3、分发kafka" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、分发kafka" class="headerlink" title="3、分发kafka"></a>3、分发kafka</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/</span><br><span class="line">syncfile /export/server/kafka_2.12-2.4.1</span><br></pre></td></tr></table></div></figure>


        <h2 id="4、配置环境变量"   >
          <a href="#4、配置环境变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、配置环境变量" class="headerlink" title="4、配置环境变量"></a>4、配置环境变量</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile </span><br><span class="line"></span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin </span><br><span class="line">source /etc/profile </span><br><span class="line">#注意:还需要分发环境变量</span><br><span class="line">syncfile /etc/profile</span><br></pre></td></tr></table></div></figure>


        <h2 id="5、分别在node2和node3上修改配置文件"   >
          <a href="#5、分别在node2和node3上修改配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#5、分别在node2和node3上修改配置文件" class="headerlink" title="5、分别在node2和node3上修改配置文件"></a>5、分别在node2和node3上修改配置文件</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim /export/server/kafka/config/server.propertie</span><br><span class="line">broker.id=1 </span><br><span class="line">broker.id=2</span><br><span class="line">#(broker.id 不能重复)</span><br><span class="line"></span><br><span class="line">#启停集群(在各个节点上启动)</span><br><span class="line">#启动集群</span><br><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties </span><br><span class="line">#停止集群</span><br><span class="line">kafka-server-stop.sh stop</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/kafka/1.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/20/kafka-eagle/">在kafka集群中部署Eagle运维监控</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-20</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-21</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>部署Kafka Eagle，完成对kafka集群的基本监控。</p>
<p>1、上传安装包到服务器中，解压到&#x2F;export&#x2F;server目录下</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/software/</span><br><span class="line">tar -zxvf kafka-eagle-web-2.0.2-bin.tar.gz -C /export/server/</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/1.png"></p>
<p>2、设置软连接</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/kafka-eagle-web-2.0.2/ /export/server/kafka-eagle</span><br></pre></td></tr></table></div></figure>

<p>3、 配置环境变量:JAVA_HOME 和 KE_HOME</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"># JAVA_HOME(之前配置过的，不用配置，会有冲突，可能无法识别)</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"># KE_HOME</span><br><span class="line">export KE_HOME=/export/server/kafka-eagle</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></div></figure>

<p>4、 配置 KafkaEagle</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka-eagle/conf </span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line">#需要更改的地方如下图，改为：</span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=node1:2181,node2:2181,node3:2181</span><br><span class="line">cluster1.kafka.eagle.broker.size=3</span><br><span class="line"></span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/export/data/db/ke.db</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/2.png"></p>
<p><img src="/../image/eagle/3.png"></p>
<p>5、启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/data/db</span><br></pre></td></tr></table></div></figure>

<p>6、启动zookeeper</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkall.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/5.png"></p>
<p>7、启动kafka集群</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kfkall.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/6.png"></p>
<p><img src="/../image/eagle/7.png"></p>
<p>8、启动Eagle</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/kafka-eagle/bin/ke.sh start</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/8.png"></p>
<p>9、网页访问Eagle</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> Account:admin </span><br><span class="line"> Password:123456</span><br><span class="line"></span><br><span class="line">http://192.168.88.151:8048</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/eagle/9.png"></p>
<p>10、查看BScreen</p>
<p><img src="/../image/eagle/10.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/14/hexo+github/">使用hexo+GitHub建立个人博客</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-20</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="一、安装Hexo"   >
          <a href="#一、安装Hexo" class="heading-link"><i class="fas fa-link"></i></a><a href="#一、安装Hexo" class="headerlink" title="一、安装Hexo"></a>一、安装Hexo</h1>
      <p>安装 Hexo 时先安装下列应用程序：</p>
<ul>
<li>Node.js (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本)</li>
<li>Git</li>
</ul>

        <h2 id="（1）git"   >
          <a href="#（1）git" class="heading-link"><i class="fas fa-link"></i></a><a href="#（1）git" class="headerlink" title="（1）git"></a>（1）git</h2>
      
        <h3 id="git下载"   >
          <a href="#git下载" class="heading-link"><i class="fas fa-link"></i></a><a href="#git下载" class="headerlink" title="git下载"></a>git下载</h3>
      <p>git的下载地址：<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://git-scm.com/download" >https://git-scm.com/download</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<p>附件：</p>
<p><img src="/../image/hexo/1.png"></p>

        <h3 id="git安装"   >
          <a href="#git安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#git安装" class="headerlink" title="git安装"></a>git安装</h3>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1、按照附件的顺序，直接下一步安装即可。</span><br><span class="line">2、安装的过程中需要填写一个邮箱和用户名(任意即可)</span><br><span class="line">3、安装完毕请重启资源管理器,或者重启电脑</span><br><span class="line">4、更改语言(选择修改)</span><br></pre></td></tr></table></div></figure>


        <h2 id="（2）nodejs"   >
          <a href="#（2）nodejs" class="heading-link"><i class="fas fa-link"></i></a><a href="#（2）nodejs" class="headerlink" title="（2）nodejs"></a>（2）nodejs</h2>
      
        <h3 id="nodejs下载和安装"   >
          <a href="#nodejs下载和安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#nodejs下载和安装" class="headerlink" title="nodejs下载和安装"></a>nodejs下载和安装</h3>
      <p>1、官方下载对应的系统安装包。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://nodejs.org/en/download/</span><br></pre></td></tr></table></div></figure>

<p>2、安装时建议修改安装路径，放在非C盘目录下，一路默认安装即可。</p>
<p>3、安装完成后启动命令行工具，输入<code>node -v</code>和<code>npm -v</code>查看安装版本，出现提示版本信息即为安装成功。</p>
<p><img src="/../image/hexo/2.png"></p>

        <h3 id="环境配置"   >
          <a href="#环境配置" class="heading-link"><i class="fas fa-link"></i></a><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3>
      <p>1、在Node.js安装目录下，如 D:\nodejs 新建两个文件夹：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node_global(全局包存放目录)</span><br><span class="line"></span><br><span class="line">node_cache(缓存目录)</span><br></pre></td></tr></table></div></figure>

<p>2、打开命令行工具，执行以下两句操作：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#设置全局包目录</span><br><span class="line">npm config set prefix &quot;D:\nodejs\node_global&quot; </span><br><span class="line">#设置缓存目录</span><br><span class="line">npm config set cache &quot;D:\nodejs\node_cache&quot;</span><br></pre></td></tr></table></div></figure>

<p>3、配置环境变量：</p>
<p>（1）、打开环境变量，在系统变量中新建</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">变量名：NODE_PATH</span><br><span class="line">变量值：D:\nodejs\node_global\node_modules</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/3.png"></p>
<p>（2）、编辑用户变量的 path</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将默认的 C 盘下 APPData\Roaming\npm 修改为 </span><br><span class="line">D:\nodejs\node_global</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/4.png"></p>

        <h3 id="切换镜像源"   >
          <a href="#切换镜像源" class="heading-link"><i class="fas fa-link"></i></a><a href="#切换镜像源" class="headerlink" title="切换镜像源"></a>切换镜像源</h3>
      <p>1、npm查看当前源：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm get registry</span><br></pre></td></tr></table></div></figure>

<p>2、npm设置淘宝镜像源：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/5.png"></p>

        <h2 id="（3）hexo"   >
          <a href="#（3）hexo" class="heading-link"><i class="fas fa-link"></i></a><a href="#（3）hexo" class="headerlink" title="（3）hexo"></a>（3）hexo</h2>
      <p>所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/6.png"></p>

        <h1 id="二、hexo搭建静态网页"   >
          <a href="#二、hexo搭建静态网页" class="heading-link"><i class="fas fa-link"></i></a><a href="#二、hexo搭建静态网页" class="headerlink" title="二、hexo搭建静态网页"></a>二、hexo搭建静态网页</h1>
      <p>Hexo是基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。</p>
<p>本地访问博客</p>
<p>执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init &lt;folder&gt;</span><br><span class="line">cd &lt;folder&gt;</span><br><span class="line">npm install     # 安装组件</span><br></pre></td></tr></table></div></figure>

<p>新建完成后，指定文件夹的目录如下：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br><span class="line"># _config.yml（网站的配置信息，可以在此配置大部分的参数）</span><br><span class="line"></span><br><span class="line"># source（资源文件夹是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。）#其中md文档放在_posts文件夹下</span><br><span class="line"></span><br><span class="line"># themes（主题文件夹。Hexo 会根据主题来生成静态页面。）</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/7.png"></p>
<p><img src="/../image/hexo/8.png"></p>
<p>执行下列命令，启动本地服务器进行预览：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g   # 生成页面</span><br><span class="line">hexo s   # 启动预览</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/9.png"></p>
<p><img src="/../image/hexo/10.png"></p>
<p>访问 <code>http://localhost:4000</code>，出现 Hexo 默认页面，本地博客安装成功</p>
<p><img src="/../image/hexo/11.png"></p>

        <h1 id="三、通过ssh连接将博客部署到github"   >
          <a href="#三、通过ssh连接将博客部署到github" class="heading-link"><i class="fas fa-link"></i></a><a href="#三、通过ssh连接将博客部署到github" class="headerlink" title="三、通过ssh连接将博客部署到github"></a>三、通过ssh连接将博客部署到github</h1>
      <p>静态页面（本地博客）已经生成，还需要将其部署到GitHub上。一来搭建的博客别人可以（以一个链接）访问到，二来需要有个托管平台，这样可以关注博客内容本身而不是麻烦的管理。</p>

        <h2 id="创建github仓库"   >
          <a href="#创建github仓库" class="heading-link"><i class="fas fa-link"></i></a><a href="#创建github仓库" class="headerlink" title="创建github仓库"></a>创建github仓库</h2>
      <p>1、注册github账户</p>
<p><img src="/../image/hexo/12.png"></p>
<p>2、新建仓库</p>
<p>创建一个与用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub page的时候，才会被识别为xxxx.github.io，其中xxx就是注册GitHub的用户名。</p>
<p><img src="/../image/hexo/13.png"></p>

        <h2 id="ssh连接"   >
          <a href="#ssh连接" class="heading-link"><i class="fas fa-link"></i></a><a href="#ssh连接" class="headerlink" title="ssh连接"></a>ssh连接</h2>
      <p>在本地通过ssh绑定github仓库，可通过 git 上传文件到 github上。</p>
<p>1、在cmd中执行以下命令，生成公钥：</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;yourname&quot;</span><br><span class="line">git config --global user.email &quot;youremail&quot;</span><br><span class="line"></span><br><span class="line">ssh-keygen -t rsa -C &quot;youremail&quot;</span><br></pre></td></tr></table></div></figure>

<p>2、将生成的公钥（id_rsa.pub的内容）复制github上</p>
<p><img src="/../image/hexo/17.png"></p>

        <h2 id="修改配置文件"   >
          <a href="#修改配置文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2>
      <p>配置<folder>目录下的_config.yml</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 配置hexo把&lt;folder&gt;部署到github仓库里</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:username/username.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></div></figure>


        <h2 id="安装hexo部署插件"   >
          <a href="#安装hexo部署插件" class="heading-link"><i class="fas fa-link"></i></a><a href="#安装hexo部署插件" class="headerlink" title="安装hexo部署插件"></a>安装hexo部署插件</h2>
      <p>安装deploy-git ，也就是部署的命令,这样才能用命令部署到GitHub。</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></div></figure>


        <h2 id="通过github-page访问Hexo博客"   >
          <a href="#通过github-page访问Hexo博客" class="heading-link"><i class="fas fa-link"></i></a><a href="#通过github-page访问Hexo博客" class="headerlink" title="通过github page访问Hexo博客"></a>通过github page访问Hexo博客</h2>
      <figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#清除生成的网页文件</span><br><span class="line">hexo clean</span><br><span class="line">#生成网页文件</span><br><span class="line">hexo g</span><br><span class="line">#上传网页文件到 Github page</span><br><span class="line">hexo d</span><br></pre></td></tr></table></div></figure>

<p>接着访问<code>https://username.github.io/</code>即可</p>

        <h1 id="四、修改主题"   >
          <a href="#四、修改主题" class="heading-link"><i class="fas fa-link"></i></a><a href="#四、修改主题" class="headerlink" title="四、修改主题"></a>四、修改主题</h1>
      <p>系统默认给的主题是 landscape，将主题修改为stun</p>
<p>1、进入你的 hexo 站点文件夹，克隆 <code>stun</code> 主题到 <code>themes/</code> 路径下</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &lt;folder&gt;</span><br><span class="line">git clone https://github.com/liuyib/hexo-theme-stun.git themes/stun</span><br></pre></td></tr></table></div></figure>

<p>2、安装依赖 <code>hexo-renderer-pug</code></p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-renderer-pug</span><br></pre></td></tr></table></div></figure>

<p>3、修改<folder>目录下的_config.yml文件</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: stun</span><br></pre></td></tr></table></div></figure>

<p>4、启动 Hexo 服务器</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#清除生成的网页文件</span><br><span class="line">hexo clean</span><br><span class="line">#生成网页文件</span><br><span class="line">hexo g</span><br><span class="line">#上传网页文件到 Github page</span><br><span class="line">hexo d</span><br></pre></td></tr></table></div></figure>

<p>5、访问<code>https://username.github.io/</code></p>

        <h1 id="五、上传md文档"   >
          <a href="#五、上传md文档" class="heading-link"><i class="fas fa-link"></i></a><a href="#五、上传md文档" class="headerlink" title="五、上传md文档"></a>五、上传md文档</h1>
      <p>1、在<folder>项目中，source文件夹下的_posts文件夹中上传md文档</p>
<p><img src="/../image/hexo/14.png"></p>
<p>2、md文档中的图片可以放在source文件夹下新建的image文件夹中</p>
<p><img src="/../image/hexo/15.png"></p>
<p>注：每次上传后都要执行以下命令，否则通过github page访问Hexo博客中的内容不更新</p>
<figure class="highlight plaintext"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#清除生成的网页文件</span><br><span class="line">hexo clean</span><br><span class="line">#生成网页文件</span><br><span class="line">hexo g</span><br><span class="line">#上传网页文件到 Github page</span><br><span class="line">hexo d</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/hexo/16.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/14/Spark(Yarn)/">Spark(Yarn)</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-14</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="1、client模式测试"   >
          <a href="#1、client模式测试" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、client模式测试" class="headerlink" title="1、client模式测试"></a>1、client模式测试</h1>
      
        <h2 id="假设运行圆周率PI程序，采用client模式，命令如下："   >
          <a href="#假设运行圆周率PI程序，采用client模式，命令如下：" class="heading-link"><i class="fas fa-link"></i></a><a href="#假设运行圆周率PI程序，采用client模式，命令如下：" class="headerlink" title="假设运行圆周率PI程序，采用client模式，命令如下："></a>假设运行圆周率PI程序，采用client模式，命令如下：</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 <span class="variable">$&#123;SPARK_HOME&#125;</span>/examples/src/main/python/pi.py 10</span></span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/52.png"></p>
<p><img src="/../image/53.png"></p>

        <h1 id="2、cluster模式测试"   >
          <a href="#2、cluster模式测试" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、cluster模式测试" class="headerlink" title="2、cluster模式测试"></a>2、cluster模式测试</h1>
      
        <h2 id="假设运行圆周率PI程序，采用cluster模式，命令如下："   >
          <a href="#假设运行圆周率PI程序，采用cluster模式，命令如下：" class="heading-link"><i class="fas fa-link"></i></a><a href="#假设运行圆周率PI程序，采用cluster模式，命令如下：" class="headerlink" title="假设运行圆周率PI程序，采用cluster模式，命令如下："></a>假设运行圆周率PI程序，采用cluster模式，命令如下：</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$</span><span class="language-bash">&#123;SPARK_HOME&#125;/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 1 --total-executor-cores 2 --conf <span class="string">&quot;spark.pyspark.driver.python=/root/anaconda3/bin/python3&quot;</span> --conf <span class="string">&quot;spark.pyspark.python=/root/anaconda3/bin/python3&quot;</span> <span class="variable">$&#123;SPARK_HOME&#125;</span>/examples/src/main/python/pi.py 10</span></span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/54.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/14/Spark(ha)/">Spark(ha)</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-14</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>1、启动Zookeeper、HDFS</p>
<p><img src="/../image/38.png"></p>
<p>2、在spark-env.sh中, 删除: SPARK_MASTER_HOST&#x3D;node1</p>
<p><img src="/../image/39.png"></p>
<p>3、在spark-env.sh中增加：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定Zookeeper的连接地址</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定在Zookeeper中注册临时节点的路径</span></span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/40.png"></p>
<p>4、将spark-env.sh 分别分发到node2、node3上</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh node2:/export/server/spark/conf/</span><br><span class="line">scp spark-env.sh node3:/export/server/spark/conf/</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/41.png"></p>
<p>5、停止当前StandAlone集群</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/stop-all.sh</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/42.png"></p>
<p><img src="/../image/43.png"></p>
<p>6、启动集群，在node1上 启动一个master 和全部worker</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/44.png"></p>
<p>在node2上启动master</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-master.sh</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/45.png"></p>
<p>7、进入node1、node2 8081端口</p>
<p><img src="/../image/46.png"></p>
<p><img src="/../image/47.png"></p>
<p>8、提交一个spark任务到当前<code>alive</code>master上:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit--masterspark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 1000</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/48.png"></p>
<p>9、kill node1中master进程</p>
<p>kill -9 11354</p>
<p><img src="/../image/49.png"></p>
<p><img src="/../image/50.png"></p>
<p>10、查看node2中master进程</p>
<p><img src="/../image/51.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/14/Spark(Local)/">Spark(Local)</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-14</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="1、环境变量"   >
          <a href="#1、环境变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、环境变量" class="headerlink" title="1、环境变量"></a>1、环境变量</h1>
      <p>配置Spark由如下5个环境变量需要设置</p>
<p>- SPARK_HOME: 表示Spark安装路径在哪里 </p>
<p>- PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 </p>
<p>- JAVA_HOME: 告知Spark Java在哪里 </p>
<p>- HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 </p>
<p>- HADOOP_HOME: 告知Spark Hadoop安装在哪里</p>
<p>这5个环境变量 都需要配置在: <code>/etc/profile</code>中</p>
<p><img src="/../image/2.png"></p>

        <h1 id="2、安装anaconda"   >
          <a href="#2、安装anaconda" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、安装anaconda" class="headerlink" title="2、安装anaconda"></a>2、安装anaconda</h1>
      
        <h2 id="（1）把资源包Anaconda3-2021-05-Linux-x86-64-sh文件放到文件夹下"   >
          <a href="#（1）把资源包Anaconda3-2021-05-Linux-x86-64-sh文件放到文件夹下" class="heading-link"><i class="fas fa-link"></i></a><a href="#（1）把资源包Anaconda3-2021-05-Linux-x86-64-sh文件放到文件夹下" class="headerlink" title="（1）把资源包Anaconda3-2021.05-Linux-x86_64.sh文件放到文件夹下"></a>（1）把资源包Anaconda3-2021.05-Linux-x86_64.sh文件放到文件夹下</h2>
      <p><img src="/../image/3.png"></p>

        <h2 id="（2）运行Anaconda3-2021-05-Linux-x86-64-sh"   >
          <a href="#（2）运行Anaconda3-2021-05-Linux-x86-64-sh" class="heading-link"><i class="fas fa-link"></i></a><a href="#（2）运行Anaconda3-2021-05-Linux-x86-64-sh" class="headerlink" title="（2）运行Anaconda3-2021.05-Linux-x86_64.sh"></a>（2）运行Anaconda3-2021.05-Linux-x86_64.sh</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ./Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/4.png"></p>
<p><img src="/../image/5.png"></p>

        <h2 id="3-出现（base）即为安装成功"   >
          <a href="#3-出现（base）即为安装成功" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-出现（base）即为安装成功" class="headerlink" title="(3)出现（base）即为安装成功"></a>(3)出现（base）即为安装成功</h2>
      <p><img src="/../image/6.png" alt="6"></p>

        <h1 id="3、创建虚拟环境"   >
          <a href="#3、创建虚拟环境" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、创建虚拟环境" class="headerlink" title="3、创建虚拟环境"></a>3、创建虚拟环境</h1>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br><span class="line">conda activate pyspark</span><br><span class="line">pipinstallpyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn</span><br><span class="line">/simple</span><br></pre></td></tr></table></div></figure>


        <h1 id="4、修改环境变量配置Spark由如下5个环境变量需要设置"   >
          <a href="#4、修改环境变量配置Spark由如下5个环境变量需要设置" class="heading-link"><i class="fas fa-link"></i></a><a href="#4、修改环境变量配置Spark由如下5个环境变量需要设置" class="headerlink" title="4、修改环境变量配置Spark由如下5个环境变量需要设置"></a>4、修改环境变量配置Spark由如下5个环境变量需要设置</h1>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HOME: 表示Spark安装路径在哪里 </span><br><span class="line"></span><br><span class="line">PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器</span><br><span class="line"></span><br><span class="line">JAVA_HOME: 告知Spark Java在哪里</span><br><span class="line"></span><br><span class="line">HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里</span><br><span class="line"></span><br><span class="line">HADOOP_HOME: 告知Spark  Hadoop安装在哪里</span><br></pre></td></tr></table></div></figure>

<p>这5个环境变量 都需要配置在:&#x2F;etc&#x2F;profile中<img src="/../image/7.png"></p>
<p>PYSPARK_PYTHON和 JAVA_HOME 需要同样配置在:</p>
<p>&#x2F;root&#x2F;.bashrc中</p>
<p><img src="/../image/8.png"></p>

        <h1 id="5、上传Spark安装包"   >
          <a href="#5、上传Spark安装包" class="heading-link"><i class="fas fa-link"></i></a><a href="#5、上传Spark安装包" class="headerlink" title="5、上传Spark安装包"></a>5、上传Spark安装包</h1>
      <p>资料中提供了: spark-3.2.0-bin-hadoop3.2.tgz</p>
<p>上传这个文件到Linux服务器中</p>
<p><img src="/../image/9.png"></p>

        <h1 id="6、解压"   >
          <a href="#6、解压" class="heading-link"><i class="fas fa-link"></i></a><a href="#6、解压" class="headerlink" title="6、解压"></a>6、解压</h1>
      <p>将其解压, 课程中将其解压(安装)到: &#x2F;export&#x2F;server内.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></div></figure>

<p>由于spark目录名称很长, 给其一个软链接:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s/export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/10.png"></p>

        <h1 id="7、测试"   >
          <a href="#7、测试" class="heading-link"><i class="fas fa-link"></i></a><a href="#7、测试" class="headerlink" title="7、测试"></a>7、测试</h1>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark</span><br></pre></td></tr></table></div></figure>

<p>bin&#x2F;pyspark 程序, 可以提供一个交互式的 Python解释器环境, 在这里面可以写普通python代码, 以及spark代码</p>
<p><img src="/../image/11.png"></p>
<p><img src="/../image/12.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/14/Spark%20(Stand%20Alone)/">Spark(Stand Alone)</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-14</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="部署Spark-Stand-Alone环境"   >
          <a href="#部署Spark-Stand-Alone环境" class="heading-link"><i class="fas fa-link"></i></a><a href="#部署Spark-Stand-Alone环境" class="headerlink" title="部署Spark Stand Alone环境"></a>部署Spark Stand Alone环境</h1>
      <p>需要用到三台linux虚拟机构成，在此以node1、node2、node3为例</p>
<p>node1运行: Spark的Master进程  和 1个Worker进程</p>
<p>node2运行: spark的1个worker进程</p>
<p>node3运行: spark的1个worker进程</p>

        <h2 id="1-安装与配置（此处在node1上操作）"   >
          <a href="#1-安装与配置（此处在node1上操作）" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-安装与配置（此处在node1上操作）" class="headerlink" title="1.安装与配置（此处在node1上操作）"></a>1.安装与配置（此处在node1上操作）</h2>
      
        <h3 id="（1）三台虚拟机都需要配置pyspark虚拟环境，以及安装虚拟环境中所需要的包。参考四中的1-基础环境"   >
          <a href="#（1）三台虚拟机都需要配置pyspark虚拟环境，以及安装虚拟环境中所需要的包。参考四中的1-基础环境" class="heading-link"><i class="fas fa-link"></i></a><a href="#（1）三台虚拟机都需要配置pyspark虚拟环境，以及安装虚拟环境中所需要的包。参考四中的1-基础环境" class="headerlink" title="（1）三台虚拟机都需要配置pyspark虚拟环境，以及安装虚拟环境中所需要的包。参考四中的1.基础环境"></a>（1）三台虚拟机都需要配置pyspark虚拟环境，以及安装虚拟环境中所需要的包。参考四中的1.基础环境</h3>
      
        <h3 id="（2）进入spark配置文件的目录中"   >
          <a href="#（2）进入spark配置文件的目录中" class="heading-link"><i class="fas fa-link"></i></a><a href="#（2）进入spark配置文件的目录中" class="headerlink" title="（2）进入spark配置文件的目录中"></a>（2）进入spark配置文件的目录中</h3>
      <p><img src="/../image/13.png"></p>

        <h3 id="（3）配置workers文件"   >
          <a href="#（3）配置workers文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#（3）配置workers文件" class="headerlink" title="（3）配置workers文件"></a>（3）配置workers文件</h3>
      
        <h4 id="1）改名-去掉后面的-template后缀"   >
          <a href="#1）改名-去掉后面的-template后缀" class="heading-link"><i class="fas fa-link"></i></a><a href="#1）改名-去掉后面的-template后缀" class="headerlink" title="1）改名, 去掉后面的.template后缀"></a>1）改名, 去掉后面的.template后缀</h4>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/14.png"></p>

        <h4 id="2）编辑workers文件"   >
          <a href="#2）编辑workers文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#2）编辑workers文件" class="headerlink" title="2）编辑workers文件"></a>2）编辑workers文件</h4>
      <p>vim workers</p>
<p>把localhost删除，在里面追加</p>
<p>node1</p>
<p>node2</p>
<p>node3</p>
<p><img src="/../image/15.png"></p>

        <h4 id="3）配置spark-env-sh文件"   >
          <a href="#3）配置spark-env-sh文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#3）配置spark-env-sh文件" class="headerlink" title="3）配置spark-env.sh文件"></a>3）配置spark-env.sh文件</h4>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/16.png"></p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">在spark-env.sh底下追加以下内容</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置JAVA安装目录</span></span></span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 指定spark老大Master的IP和提交任务的通信端口</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知Spark的master运行在哪个机器上,node1上启动master</span></span><br><span class="line">export SPARK_MASTER_HOST=node1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知spark master的通讯端口</span></span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置历史服务器</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/17.png"></p>

        <h4 id="4）在HDFS上创建用来存放程序运行历史记录的文件夹"   >
          <a href="#4）在HDFS上创建用来存放程序运行历史记录的文件夹" class="heading-link"><i class="fas fa-link"></i></a><a href="#4）在HDFS上创建用来存放程序运行历史记录的文件夹" class="headerlink" title="4）在HDFS上创建用来存放程序运行历史记录的文件夹"></a>4）在HDFS上创建用来存放程序运行历史记录的文件夹</h4>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/18.png"></p>
<p><img src="/../image/19.png"></p>

        <h4 id="5）配置spark-defaults-conf"   >
          <a href="#5）配置spark-defaults-conf" class="heading-link"><i class="fas fa-link"></i></a><a href="#5）配置spark-defaults-conf" class="headerlink" title="5）配置spark-defaults.conf"></a>5）配置spark-defaults.conf</h4>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/20.png"></p>
<p>在文件中追加以下内容</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启spark的日期记录功能</span></span><br><span class="line"></span><br><span class="line">spark.eventLog.enabled     true</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志记录的路径</span></span><br><span class="line"></span><br><span class="line">spark.eventLog.dir  hdfs://node1:8020/sparklog/ </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志是否启动压缩</span></span><br><span class="line"></span><br><span class="line">spark.eventLog.compress    true</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/21.png"></p>

        <h4 id="6）配置log4j文件"   >
          <a href="#6）配置log4j文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#6）配置log4j文件" class="headerlink" title="6）配置log4j文件"></a>6）配置log4j文件</h4>
      <p>更改INFO为WARN</p>
<p><img src="/../image/22.png"></p>

        <h2 id="2-分发"   >
          <a href="#2-分发" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-分发" class="headerlink" title="2.分发"></a>2.分发</h2>
      
        <h3 id="（1）将node1上配置好的Spark分发到其他文件夹上"   >
          <a href="#（1）将node1上配置好的Spark分发到其他文件夹上" class="heading-link"><i class="fas fa-link"></i></a><a href="#（1）将node1上配置好的Spark分发到其他文件夹上" class="headerlink" title="（1）将node1上配置好的Spark分发到其他文件夹上"></a>（1）将node1上配置好的Spark分发到其他文件夹上</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r spark-3.2.0-bin-hadoop3.2/ root@node2:/export/server/</span><br><span class="line">scp -r spark-3.2.0-bin-hadoop3.2/ root@node3:/export/server/</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/23.png"></p>
<p><img src="/../image/24.png"></p>

        <h3 id="（2）在node2和node3上设置软链接"   >
          <a href="#（2）在node2和node3上设置软链接" class="heading-link"><i class="fas fa-link"></i></a><a href="#（2）在node2和node3上设置软链接" class="headerlink" title="（2）在node2和node3上设置软链接"></a>（2）在node2和node3上设置软链接</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s spark-3.2.0-bin-hadoop3.2/ spark</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/25.png"></p>
<p><img src="/../image/26.png"></p>

        <h2 id="3-配置三台虚拟机的-x2F-etc-x2F-profile文件"   >
          <a href="#3-配置三台虚拟机的-x2F-etc-x2F-profile文件" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-配置三台虚拟机的-x2F-etc-x2F-profile文件" class="headerlink" title="3.配置三台虚拟机的&#x2F;etc&#x2F;profile文件"></a>3.配置三台虚拟机的&#x2F;etc&#x2F;profile文件</h2>
      <p><img src="/../image/27.png"></p>
<p><img src="/../image/28.png"></p>

        <h2 id="4-启动spark的master和worker进程"   >
          <a href="#4-启动spark的master和worker进程" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-启动spark的master和worker进程" class="headerlink" title="4.启动spark的master和worker进程"></a>4.启动spark的master和worker进程</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/29.png"></p>

        <h2 id="5-查看进程"   >
          <a href="#5-查看进程" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-查看进程" class="headerlink" title="5.查看进程"></a>5.查看进程</h2>
      <p>node1：</p>
<p><img src="/../image/30.png"></p>
<p>node2：</p>
<p><img src="/../image/31.png"></p>
<p>node3：</p>
<p><img src="/../image/32.png"></p>

        <h2 id="6-查看master的web-ui"   >
          <a href="#6-查看master的web-ui" class="heading-link"><i class="fas fa-link"></i></a><a href="#6-查看master的web-ui" class="headerlink" title="6.查看master的web ui"></a>6.查看master的web ui</h2>
      <p>因为spark配置过程中，默认端口master设置了8080，所以登录到node1:8080网址上</p>
<p><img src="/../image/33.png"></p>

        <h2 id="7-链接到StandAlone集群"   >
          <a href="#7-链接到StandAlone集群" class="heading-link"><i class="fas fa-link"></i></a><a href="#7-链接到StandAlone集群" class="headerlink" title="7.链接到StandAlone集群"></a>7.链接到StandAlone集群</h2>
      
        <h3 id="（1）pyspark"   >
          <a href="#（1）pyspark" class="heading-link"><i class="fas fa-link"></i></a><a href="#（1）pyspark" class="headerlink" title="（1）pyspark"></a>（1）pyspark</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./pyspark --master spark://node1:7077</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/34.png"></p>

        <h3 id="（2）spark-shell"   >
          <a href="#（2）spark-shell" class="heading-link"><i class="fas fa-link"></i></a><a href="#（2）spark-shell" class="headerlink" title="（2）spark-shell"></a>（2）spark-shell</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1）./ spark-shell --master spark://node1:7077</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/35.png"></p>

        <h3 id="（3）spark-submit-PI"   >
          <a href="#（3）spark-submit-PI" class="heading-link"><i class="fas fa-link"></i></a><a href="#（3）spark-submit-PI" class="headerlink" title="（3）spark-submit (PI)"></a>（3）spark-submit (PI)</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 100</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/36.png"></p>

        <h2 id="8-查看历史服务器web-ui"   >
          <a href="#8-查看历史服务器web-ui" class="heading-link"><i class="fas fa-link"></i></a><a href="#8-查看历史服务器web-ui" class="headerlink" title="8.查看历史服务器web ui"></a>8.查看历史服务器web ui</h2>
      <p>打开浏览器</p>
<p><img src="/../image/37.png"></p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2023/06/14/Spark(pyspark)/">Spark(Pyspark)</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2023-06-14</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2023-06-14</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h1 id="1、Pyspark环境配置安装"   >
          <a href="#1、Pyspark环境配置安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#1、Pyspark环境配置安装" class="headerlink" title="1、Pyspark环境配置安装"></a>1、Pyspark环境配置安装</h1>
      <p>PySpark是Spark官方提供的一个Python类库，内置了Spark API，可以通过PySpark类库来编写Spark程序，并提交到Spark集群中运行。</p>
<p>准备工作：</p>
<p>（1）将课程资料中提供的的hadoop-3.3.0文件，复制到某个盘符下（中文的且无空格的）。</p>
<p>（2）将文件夹内bin内的Hadoop.dll复制到C:\Windows\Systmctl32里面去。</p>
<p>（3）在系统环境变量中配置HADOOP_HOME，指向hadoop-3.3.0文件夹的路径。</p>
<p><img src="/../image/pyspark/1.png"></p>

        <h1 id="2、本机PySpark环境配置"   >
          <a href="#2、本机PySpark环境配置" class="heading-link"><i class="fas fa-link"></i></a><a href="#2、本机PySpark环境配置" class="headerlink" title="2、本机PySpark环境配置"></a>2、本机PySpark环境配置</h1>
      <p>在前面部署Spark的时候，已经在Linux系统上部署了acaconda的Python环境，故本次在Windows上安装anaconda，并配置PySpark库。</p>

        <h2 id="2-1-在课程资料中选择anaconda应用程序双击安装"   >
          <a href="#2-1-在课程资料中选择anaconda应用程序双击安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-在课程资料中选择anaconda应用程序双击安装" class="headerlink" title="2.1.在课程资料中选择anaconda应用程序双击安装"></a>2.1.在课程资料中选择anaconda应用程序双击安装</h2>
      <p><img src="/../image/pyspark/2.png"></p>

        <h2 id="2-2-一直选择Next，进行安装"   >
          <a href="#2-2-一直选择Next，进行安装" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-一直选择Next，进行安装" class="headerlink" title="2.2.一直选择Next，进行安装"></a>2.2.一直选择Next，进行安装</h2>
      <p><img src="/../image/pyspark/3.png"></p>
<p><img src="/../image/pyspark/4.png"></p>

        <h2 id="2-3-安装结束后会出现anaconda3文件夹。打开Anaconda-Prompt-anaconda-会出现base，即为安装成功。"   >
          <a href="#2-3-安装结束后会出现anaconda3文件夹。打开Anaconda-Prompt-anaconda-会出现base，即为安装成功。" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-3-安装结束后会出现anaconda3文件夹。打开Anaconda-Prompt-anaconda-会出现base，即为安装成功。" class="headerlink" title="2.3.安装结束后会出现anaconda3文件夹。打开Anaconda Prompt(anaconda),会出现base，即为安装成功。"></a>2.3.安装结束后会出现anaconda3文件夹。打开Anaconda Prompt(anaconda),会出现base，即为安装成功。</h2>
      <p><img src="/../image/pyspark/5.png"></p>
<p><img src="/../image/pyspark/6.png"></p>

        <h1 id="3、配置国内源，加速网络下载"   >
          <a href="#3、配置国内源，加速网络下载" class="heading-link"><i class="fas fa-link"></i></a><a href="#3、配置国内源，加速网络下载" class="headerlink" title="3、配置国内源，加速网络下载"></a>3、配置国内源，加速网络下载</h1>
      <p>在Anaconda Prompt(anaconda)中执行conda config –set show_channel<br>_urls yes。<br>将如下内容替换到C:\Users\用户名.condarc文件中。</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">- defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line">- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></div></figure>

<p>创建虚拟环境</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">创建虚拟环境 pyspark, 基于Python 3.8</span><br><span class="line">conda create -n pyspark python=3.8</span><br><span class="line">切换到虚拟环境内</span><br><span class="line">conda activate pyspark</span><br><span class="line">在虚拟环境内安装包</span><br><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></div></figure>

<p><img src="/../image/pyspark/7.png"></p>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">Hello Stun</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">Archives</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>John Doe</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v6.3.0</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.8.0</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>